{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing Classification with Cost-Based Thresholding\n",
    "This notebook combines the study of bank marketing classification models with cost-based thresholding evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "Every project has its own dependancies. Libraries, settings, tools they all serve a purpose and its good practise to add this at a centralized place within the code. There is no better place than the starting section of a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# necesarry for exporting the model\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "As mentioned in Geron importing data with a function is a good practise.\n",
    "By adding this functionality, it makes the code easier to refactor into production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading with function for easy maintanance later on (best practices as Geron discussed)\n",
    "def getDataSet():\n",
    "    return pd.read_csv(\"bank-additional-full.csv\", sep=';')\n",
    "\n",
    "# Load dataset\n",
    "df = getDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data\n",
    "Preprocessing data is necesarry as it makes the dataset more readable and usable for models. Some values have little to no meaning in their original form, but preprocessing enables the data to have meaning. Mapping of values, removing data that is lacking or one-hot encoding all happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (41188, 20)\n",
      "Target distribution: y\n",
      "0    0.887346\n",
      "1    0.112654\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Drop duration (not known before call is made)\n",
    "df.drop(columns=['duration'], inplace=True)\n",
    "\n",
    "# Encode target variable\n",
    "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].replace('unknown', np.nan)\n",
    "for col in categorical_cols:\n",
    "   df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# One-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Train-test split\n",
    "X = df_encoded.drop(columns=['y'])\n",
    "y = df_encoded['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection\n",
    "This code sets up a feature selection pipeline using Recursive Feature Elimination (RFE) with a logistic regression model. The LogisticRegression is configured with the 'liblinear' solver and a maximum of 1000 iterations, and RFE will iteratively remove features one at a time (step=1) until only the top 20 most important features (n_features_to_select=20) remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (20): ['previous', 'euribor3m', 'job_blue-collar', 'job_retired', 'job_services', 'job_student', 'education_basic.9y', 'education_university.degree', 'contact_telephone', 'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_mar', 'month_may', 'month_oct', 'day_of_week_mon', 'day_of_week_wed', 'poutcome_nonexistent', 'poutcome_success']\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using RFE\n",
    "lr_model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "selector = RFE(estimator=lr_model, n_features_to_select=20, step=1)\n",
    "selector.fit(X_train, y_train)\n",
    "selected_features = X_train.columns[selector.support_]\n",
    "\n",
    "# Prepare reduced feature set\n",
    "X_train_sel = X_train[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n",
    "\n",
    "print(f\"Selected features ({len(selected_features)}): {list(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Cost-Based Thresholding Functions\n",
    "This code implements an empirical cost-based thresholding method for binary classification, which identifies the optimal decision threshold that minimizes the expected cost of false positives and false negatives, given their respective costs (C_FP, C_FN). It calculates true positives, false positives, and false negatives across possible thresholds and plots the total cost as a function of threshold. The function also prints key performance metrics at the optimal threshold and optionally compares it with a threshold derived from cost-based probability calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_clf_curve(y_true, y_score):\n",
    "    \"\"\"Calculate thresholds as well as true and false positives and false negatives per threshold.\"\"\"\n",
    "    \n",
    "    # Ensure both inputs are NumPy arrays\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "\n",
    "    # sort scores and corresponding truth values\n",
    "    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n",
    "    y_score = y_score[desc_score_indices]\n",
    "    y_true = y_true[desc_score_indices]\n",
    "\n",
    "    # extract thresholds\n",
    "    distinct_value_indices = np.where(np.diff(y_score))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n",
    "\n",
    "    # compute metrics\n",
    "    tps = np.cumsum(y_true)[threshold_idxs]\n",
    "    fps = 1 + threshold_idxs - tps\n",
    "    fns = tps[-1] - tps\n",
    "    thresholds = y_score[threshold_idxs]\n",
    "    \n",
    "    return tps, fps, fns, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_costbased_thresholding(y_true, scores, model_name, C_FP, C_FN, p=False):\n",
    "    \"\"\"\n",
    "    Illustrates Empirical Cost-Based Thresholding\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives, false_positives, false_negatives, thresholds = binary_clf_curve(y_true, scores)\n",
    "    \n",
    "    total_costs = C_FP * false_positives + C_FN * false_negatives  \n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(thresholds, total_costs, marker='o', color='blue', linestyle='-', alpha=0.6)\n",
    "    plt.plot(thresholds, total_costs, linestyle='-', color='blue', linewidth=4, label='Total Expected Cost')\n",
    "    \n",
    "    if(p):\n",
    "        plt.xlabel(\"Probability Threshold\", fontsize=12)\n",
    "    else:\n",
    "        plt.xlabel(\"Threshold\", fontsize=12)\n",
    "    plt.ylabel(\"Cost\", fontsize=12)\n",
    "    plt.title(f\"Cost vs. Threshold - {model_name}\", fontsize=14)\n",
    "    \n",
    "    min_cost_index = np.argmin(total_costs)\n",
    "    optimal_threshold = thresholds[min_cost_index]\n",
    "    \n",
    "    plt.axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.3f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    TP = true_positives[min_cost_index]\n",
    "    FP = false_positives[min_cost_index]\n",
    "    FN = false_negatives[min_cost_index]\n",
    "    TN = len(y_true) - TP - FP - FN\n",
    "    precision = TP/(TP+FP) if (TP+FP) > 0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN) > 0 else 0\n",
    "    \n",
    "    threshold_with_calibrated_p = C_FP/(C_FN+C_FP)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"Empirically minimized expected costs: {total_costs[min_cost_index]:.2f}\")\n",
    "    \n",
    "    if(p):\n",
    "        print(f\"\\nWith calibrated p threshold would be: {threshold_with_calibrated_p:.3f}\")\n",
    "        index = np.searchsorted(-thresholds, -threshold_with_calibrated_p, side='right') - 1\n",
    "        if 0 <= index < len(total_costs):\n",
    "            print(f\"Expected cost with calibrated p threshold: {total_costs[index]:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'min_cost': total_costs[min_cost_index],\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define and Train Models\n",
    "This code trains multiple classification models on selected training features and stores their predicted probabilities on the test set for comparison and evaluation. It also returns when a model is finished as it can take some time without proper feedback when running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained: Logistic Regression\n",
      "Trained: Decision Tree\n",
      "Trained: Neural Network (MLP)\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear', max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Neural Network (MLP)\": MLPClassifier(hidden_layer_sizes=(6,), max_iter=100, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Train models and store predictions\n",
    "model_predictions = {}\n",
    "model_objects = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_sel, y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test_sel)[:, 1]\n",
    "    model_predictions[name] = y_pred_prob\n",
    "    model_objects[name] = model\n",
    "    print(f\"Trained: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Traditional Model Evaluation - ROC Curves\n",
    "In the original Moro et al. study the models where evaluated with ROC curves, so I do that here as well.\n",
    "Results are printed out to see, of course I add the visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models with ROC curves\n",
    "auc_scores = {}\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, y_pred_prob in model_predictions.items():\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_scores[name] = auc\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\")\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(\"ROC Curves for All Classifiers\", fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display AUC scores\n",
    "print(\"\\nAUC Scores:\")\n",
    "for model, auc in sorted(auc_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model}: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lift Curve Analysis\n",
    "In the original Moro et al. study the models where also evaluated with Lift curves, so I do that here as well\n",
    "Results are printed out to see, of course I add the visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Cumulative Gain (Gain Chart)\n",
    "Cumulative gain measures the total proportion of actual positives captured as you move down the ranked list of predictions. It shows how many true positives are recovered by contacting, say, the top 20% of predictions. The curve is created by sorting predicted probabilities, calculating the cumulative sum of true positives, and dividing by the total number of positives. While it helps visualize how early the model finds positives, it does not account for precision or relative performance—unlike true lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lift_curve(y_true, y_scores, label, ax=None):\n",
    "    df_lift = pd.DataFrame({'y_true': y_true, 'y_scores': y_scores})\n",
    "    df_lift = df_lift.sort_values('y_scores', ascending=False)\n",
    "    df_lift['cum_positives'] = df_lift['y_true'].cumsum()\n",
    "    df_lift['lift'] = df_lift['cum_positives'] / df_lift['y_true'].sum()\n",
    "    df_lift['percentile'] = np.arange(1, len(df_lift) + 1) / len(df_lift)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.plot(df_lift['percentile'], df_lift['lift'], label=label, linewidth=2)\n",
    "    return ax\n",
    "\n",
    "# Plot lift curves\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for name, probs in model_predictions.items():\n",
    "    plot_lift_curve(y_test, probs, label=name, ax=ax)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label=\"Baseline\", linewidth=2)\n",
    "ax.set_xlabel(\"Fraction of Sample Contacted\", fontsize=12)\n",
    "ax.set_ylabel(\"Cumulative % of Responders Captured\", fontsize=12)\n",
    "ax.set_title(\"Lift Curves for All Classifiers\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 True Lift Curve Analysis\n",
    "True lift measures how much better a model is at identifying positives in a specific segment (e.g., top 10%) compared to random selection. It is calculated as the ratio of precision at a given cutoff to the overall positive rate (baseline rate). True lift is especially useful for evaluating model performance in targeted campaigns where resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Lift Chart\n",
    "def plot_true_lift_curve(y_true, y_scores, label, ax=None, num_bins=10):\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_scores': y_scores})\n",
    "    df = df.sort_values('y_scores', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    df['bin'] = pd.qcut(df.index, q=num_bins, labels=False)\n",
    "\n",
    "    lift_values = []\n",
    "    percents = []\n",
    "    base_rate = df['y_true'].mean()\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        bin_df = df[df['bin'] == i]\n",
    "        precision_at_bin = bin_df['y_true'].mean()\n",
    "        lift = precision_at_bin / base_rate\n",
    "        lift_values.append(lift)\n",
    "        percents.append((i + 1) / num_bins)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.plot(percents, lift_values, marker='o', label=label)\n",
    "    return ax\n",
    "\n",
    "# Plot true lift curves\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for name, probs in model_predictions.items():\n",
    "    plot_true_lift_curve(y_test, probs, label=name, ax=ax)\n",
    "\n",
    "ax.axhline(1, color='k', linestyle='--', linewidth=2, label=\"Baseline Lift\")\n",
    "ax.set_xlabel(\"Fraction of Sample Contacted\", fontsize=12)\n",
    "ax.set_ylabel(\"Lift (Precision / Baseline Rate)\", fontsize=12)\n",
    "ax.set_title(\"True Lift Chart for All Classifiers\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost-Based Thresholding Analysis\n",
    "### Define Cost Parameters for Strategy Context\n",
    "To provide scenario based cost evaluation different strategies are created:\n",
    "\n",
    "Scenario 1: Conservative - Missing potential customers is more costly\n",
    "Scenario 2: Balanced\n",
    "Scenario 3: Aggressive - Making unnecessary calls is more costly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost parameters for bank marketing context\n",
    "# C_FP: Cost of calling someone who won't subscribe (wasted call)\n",
    "# C_FN: Cost of not calling someone who would have subscribed (lost opportunity)\n",
    "\n",
    "# Scenario 1: Conservative - Missing potential customers is more costly\n",
    "C_FP_conservative = 1   # Cost of unnecessary call\n",
    "C_FN_conservative = 10  # Cost of missed opportunity\n",
    "\n",
    "# Scenario 2: Balanced\n",
    "C_FP_balanced = 1\n",
    "C_FN_balanced = 5\n",
    "\n",
    "# Scenario 3: Aggressive - Making unnecessary calls is more costly\n",
    "C_FP_aggressive = 2\n",
    "C_FN_aggressive = 1\n",
    "\n",
    "scenarios = [\n",
    "    (\"Conservative\", C_FP_conservative, C_FN_conservative),\n",
    "    (\"Balanced\", C_FP_balanced, C_FN_balanced),\n",
    "    (\"Aggressive\", C_FP_aggressive, C_FN_aggressive)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_scores(scores, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Ensure scores is a 1D array-like of floats.\n",
    "    - If DataFrame with one column: squeeze to Series\n",
    "    - If DataFrame with >1 columns: error\n",
    "    - If ndarray with >1 dims: ravel to 1D\n",
    "    - Otherwise: convert to numpy array\n",
    "    \"\"\"\n",
    "    # DataFrame\n",
    "    if isinstance(scores, pd.DataFrame):\n",
    "        if scores.shape[1] == 1:\n",
    "            return scores.iloc[:, 0]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"{model_name}: Expected scores DataFrame with 1 column, \"\n",
    "                f\"but got {scores.shape[1]} columns.\"\n",
    "            )\n",
    "    # Series: OK as-is\n",
    "    if isinstance(scores, pd.Series):\n",
    "        return scores\n",
    "\n",
    "    # NumPy array\n",
    "    if isinstance(scores, np.ndarray):\n",
    "        if scores.ndim > 1:\n",
    "            return scores.ravel()\n",
    "        return scores\n",
    "\n",
    "    # Other array-like (e.g., list)\n",
    "    arr = np.array(scores)\n",
    "    if arr.ndim > 1:\n",
    "        arr = arr.ravel()\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Apply Cost-Based Thresholding to All Models\n",
    "This code evaluates all trained models across multiple cost scenarios by applying an empirical cost-based thresholding method, kindly donated by Wim, to determine the optimal classification threshold for each model-scenario combination, taking into account the specific costs of false positives and false negatives. It prints results for each scenario and stores the detailed performance metrics in a structured dictionary for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all models and scenarios\n",
    "cost_results = {}\n",
    "\n",
    "# loop thru all models n scenarios\n",
    "for scenario_name, C_FP, C_FN in scenarios:\n",
    "\n",
    "    # print out some nice header like thingy\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SCENARIO: {scenario_name} (C_FP={C_FP}, C_FN={C_FN})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # placeholder for scenario data\n",
    "    scenario_results = {}\n",
    "\n",
    "    # loop thru models and apply empirical_costbased_thresholding\n",
    "    for model_name, predictions in model_predictions.items():\n",
    "\n",
    "        # ensure that scores are 1d arrays\n",
    "        preds = sanitize_scores(predictions, model_name)\n",
    "        \n",
    "        # Check if model outputs probabilities\n",
    "        is_prob = model_name in [\"Logistic Regression\", \"Neural Network (MLP)\", \n",
    "                                 \"Random Forest\", \"Gradient Boosting\", \"Naive Bayes\", \n",
    "                                 \"XGBoost\", \"Support Vector Machine\"]\n",
    "\n",
    "        # finally I'm gonna use Wim's method, with a little tweak on the numpy arrays\n",
    "        results = empirical_costbased_thresholding(\n",
    "            y_test, preds, model_name, C_FP, C_FN, p=is_prob\n",
    "        )\n",
    "\n",
    "        # log the results with their respective model name\n",
    "        scenario_results[model_name] = results\n",
    "    \n",
    "    cost_results[scenario_name] = scenario_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparative Analysis - Cost Summary\n",
    "After all this, comparing is key, each model is ranked within one of the three scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for scenario_name, scenario_results in cost_results.items():\n",
    "    for model_name, results in scenario_results.items():\n",
    "        summary_data.append({\n",
    "            'Scenario': scenario_name,\n",
    "            'Model': model_name,\n",
    "            'Min Cost': results['min_cost'],\n",
    "            'Optimal Threshold': results['optimal_threshold'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display best model for each scenario\n",
    "print(\"\\nBest Model for Each Scenario (Lowest Cost):\")\n",
    "print(\"=\"*60)\n",
    "for scenario in scenarios:\n",
    "    scenario_name = scenario[0]\n",
    "    best_model = summary_df[summary_df['Scenario'] == scenario_name].nsmallest(1, 'Min Cost')\n",
    "    print(f\"\\n{scenario_name} Scenario:\")\n",
    "    print(f\"  Model: {best_model['Model'].values[0]}\")\n",
    "    print(f\"  Min Cost: {best_model['Min Cost'].values[0]:.2f}\")\n",
    "    print(f\"  Threshold: {best_model['Optimal Threshold'].values[0]:.3f}\")\n",
    "    print(f\"  Precision: {best_model['Precision'].values[0]:.3f}\")\n",
    "    print(f\"  Recall: {best_model['Recall'].values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization of Cost Comparison Across Scenarios\n",
    "Text is boring so lets make some cool graphs to compare as well, costs model scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (scenario_name, _, _) in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    scenario_data = summary_df[summary_df['Scenario'] == scenario_name].sort_values('Min Cost')\n",
    "    \n",
    "    bars = ax.bar(range(len(scenario_data)), scenario_data['Min Cost'])\n",
    "    ax.set_xticks(range(len(scenario_data)))\n",
    "    ax.set_xticklabels(scenario_data['Model'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Minimum Cost', fontsize=12)\n",
    "    ax.set_title(f'{scenario_name} Scenario', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Color best model differently\n",
    "    min_idx = scenario_data['Min Cost'].idxmin()\n",
    "    bars[0].set_color('green')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Precision-Recall Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create precision-recall trade-off visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (scenario_name, _, _) in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    scenario_data = summary_df[summary_df['Scenario'] == scenario_name]\n",
    "    \n",
    "    scatter = ax.scatter(scenario_data['Recall'], scenario_data['Precision'], \n",
    "                        s=200, alpha=0.6, c=range(len(scenario_data)), cmap='viridis')\n",
    "    \n",
    "    # Annotate points with model names\n",
    "    for _, row in scenario_data.iterrows():\n",
    "        ax.annotate(row['Model'][:3], (row['Recall'], row['Precision']), \n",
    "                   fontsize=8, ha='center')\n",
    "    \n",
    "    ax.set_xlabel('Recall', fontsize=12)\n",
    "    ax.set_ylabel('Precision', fontsize=12)\n",
    "    ax.set_title(f'{scenario_name} Scenario - Precision vs Recall', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cost vs AUC Analysis\n",
    "Moro et al. mentioned AUC analysis not really following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between AUC and minimum cost\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (scenario_name, _, _) in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    scenario_data = summary_df[summary_df['Scenario'] == scenario_name].copy()\n",
    "    \n",
    "    # Add AUC scores\n",
    "    scenario_data['AUC'] = scenario_data['Model'].map(auc_scores)\n",
    "    \n",
    "    scatter = ax.scatter(scenario_data['AUC'], scenario_data['Min Cost'], \n",
    "                        s=200, alpha=0.6, c=range(len(scenario_data)), cmap='plasma')\n",
    "    \n",
    "    # Annotate points\n",
    "    for _, row in scenario_data.iterrows():\n",
    "        ax.annotate(row['Model'][:3], (row['AUC'], row['Min Cost']), \n",
    "                   fontsize=15, ha='center')\n",
    "    \n",
    "    ax.set_xlabel('AUC Score', fontsize=15)\n",
    "    ax.set_ylabel('Minimum Cost', fontsize=15)\n",
    "    ax.set_title(f'{scenario_name} Scenario - Cost vs AUC', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "legend_elements = []\n",
    "\n",
    "for idx, (scenario_name, _, _) in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    scenario_data = summary_df[summary_df['Scenario'] == scenario_name].copy()\n",
    "\n",
    "    # Add AUC scores\n",
    "    scenario_data['AUC'] = scenario_data['Model'].map(auc_scores)\n",
    "\n",
    "    scatter = ax.scatter(scenario_data['AUC'], scenario_data['Min Cost'], \n",
    "                         s=200, alpha=0.6, c=range(len(scenario_data)), cmap='plasma')\n",
    "    \n",
    "    # Store color info for legend\n",
    "    for i, (_, row) in enumerate(scenario_data.iterrows()):\n",
    "        color = scatter.to_rgba(i)\n",
    "        label = row['Model']\n",
    "        legend_elements.append((label, color))\n",
    "\n",
    "    ax.set_xlabel('AUC Score', fontsize=15)\n",
    "    ax.set_ylabel('Minimum Cost', fontsize=15)\n",
    "    ax.set_title(f'{scenario_name} Scenario - Cost vs AUC', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create a custom legend below all plots\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "legend_cleaned = []\n",
    "for label, color in legend_elements:\n",
    "    if label not in seen:\n",
    "        seen.add(label)\n",
    "        legend_cleaned.append(Line2D([0], [0], marker='o', color='w', label=label,\n",
    "                                     markerfacecolor=color, markersize=10))\n",
    "\n",
    "fig.legend(handles=legend_cleaned, loc='lower center', ncol=len(legend_cleaned), fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 1])  # Make space for legend at bottom\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Model Recommendation\n",
    "Moment of truth lots of additional stuff has been done, which model is the best, and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "print(\"\\nComprehensive Model Performance Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add AUC to summary\n",
    "summary_df['AUC'] = summary_df['Model'].map(auc_scores)\n",
    "\n",
    "# Display sorted by scenario and cost\n",
    "for scenario_name, _, _ in scenarios:\n",
    "    print(f\"\\n{scenario_name} Scenario:\")\n",
    "    scenario_data = summary_df[summary_df['Scenario'] == scenario_name].sort_values('Min Cost')\n",
    "    print(scenario_data[['Model', 'Min Cost', 'Optimal Threshold', 'Precision', 'Recall', 'AUC']].to_string(index=False))\n",
    "\n",
    "# Overall best model considering all scenarios\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate average rank across scenarios\n",
    "model_ranks = {}\n",
    "for model in models.keys():\n",
    "    ranks = []\n",
    "    for scenario_name, _, _ in scenarios:\n",
    "        scenario_data = summary_df[summary_df['Scenario'] == scenario_name].sort_values('Min Cost')\n",
    "        rank = scenario_data[scenario_data['Model'] == model].index[0] + 1\n",
    "        ranks.append(rank)\n",
    "    model_ranks[model] = np.mean(ranks)\n",
    "\n",
    "# Sort by average rank\n",
    "sorted_models = sorted(model_ranks.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nModels ranked by average performance across all scenarios:\")\n",
    "for i, (model, avg_rank) in enumerate(sorted_models[:3], 1):\n",
    "    print(f\"{i}. {model} (Average Rank: {avg_rank:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cost Savings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cost savings compared to default threshold (0.5)\n",
    "print(\"\\nCost Savings Analysis (compared to default 0.5 threshold)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for scenario_name, C_FP, C_FN in scenarios:\n",
    "    print(f\"\\n{scenario_name} Scenario (C_FP={C_FP}, C_FN={C_FN}):\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for model_name, predictions in model_predictions.items():\n",
    "        # Calculate cost at default threshold 0.5\n",
    "        default_predictions = (predictions >= 0.5).astype(int)\n",
    "        FP_default = np.sum((default_predictions == 1) & (y_test == 0))\n",
    "        FN_default = np.sum((default_predictions == 0) & (y_test == 1))\n",
    "        cost_default = C_FP * FP_default + C_FN * FN_default\n",
    "        \n",
    "        # Get optimized cost\n",
    "        optimized_cost = cost_results[scenario_name][model_name]['min_cost']\n",
    "        \n",
    "        # Calculate savings\n",
    "        savings = cost_default - optimized_cost\n",
    "        savings_pct = (savings / cost_default) * 100 if cost_default > 0 else 0\n",
    "        \n",
    "        print(f\"{model_name:25s}: Savings = {savings:6.0f} ({savings_pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Exporting the models\n",
    "exporting the models to use in gradio ui later on, this step needs to be done once, can be removed later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_top_models(model_objects, summary_df, export_dir='saved_models'):\n",
    "    \"\"\"\n",
    "    Save top 3 models by average rank across all scenarios to .pkl files.\n",
    "\n",
    "    Parameters:\n",
    "    - model_objects (dict): Trained models with their names as keys.\n",
    "    - summary_df (pd.DataFrame): Summary DataFrame including cost metrics.\n",
    "    - export_dir (str): Directory to save models.\n",
    "    \"\"\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # Rank models by average performance across all scenarios\n",
    "    model_ranks = {}\n",
    "    for model in model_objects.keys():\n",
    "        model_costs = summary_df[summary_df['Model'] == model]\n",
    "        if not model_costs.empty:\n",
    "            avg_rank = model_costs['Min Cost'].rank().mean()\n",
    "            model_ranks[model] = avg_rank\n",
    "\n",
    "    # Sort models by rank and select top 8\n",
    "    top_models = sorted(model_ranks.items(), key=lambda x: x[1])[:8]\n",
    "\n",
    "    for model_name, _ in top_models:\n",
    "        model = model_objects[model_name]\n",
    "        filename = f\"{model_name.replace(' ', '_')}-v1.pkl\"\n",
    "        path = os.path.join(export_dir, filename)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Exported: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_top_models(model_objects, summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
